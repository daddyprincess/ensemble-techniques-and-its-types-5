{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "903d4829-d528-41a5-abb0-1537644a1ecc",
   "metadata": {},
   "source": [
    "## Q1. You are work#ng on a mach#ne learn#ng project where you have a dataset conta#n#ng numer#cal and categor#cal features. You have #dent#f#ed that some of the features are h#ghly correlated and there are m#ss#ng values #n some of the columns. You want to bu#ld a p#pel#ne that automates the feature eng#neer#ng process and handles the m#ss#ng valuesD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86be78c-483a-4f42-a540-c8e1e53b1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Building a pipeline that automates feature engineering and handles missing values is a crucial step in a machine learning\n",
    "project. Here's how you can design such a pipeline:\n",
    "\n",
    "Step 1: Import Necessary Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Step 2: Load and Split Data\n",
    "\n",
    "Load your dataset and split it into training and testing sets. Replace 'your_data.csv' with the actual path to your dataset\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = data.drop('target_column', axis=1)\n",
    "y = data['target_column']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "Step 3: Define Preprocessing Steps\n",
    "\n",
    "Define preprocessing steps for numerical and categorical features separately. Use ColumnTransformer to combine these steps.\n",
    "\n",
    "# Define preprocessing for numerical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with the mean\n",
    "    ('scaler', StandardScaler())  # Scale the numerical features\n",
    "])\n",
    "\n",
    "# Define preprocessing for categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent value\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical features\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps for both numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),  # Provide a list of numerical feature column names\n",
    "        ('cat', categorical_transformer, categorical_cols)  # Provide a list of categorical feature column names\n",
    "    ])\n",
    "\n",
    "\n",
    "Step 4: Feature Selection (Optional)\n",
    "\n",
    "You can include a feature selection step if needed. Here, SelectKBest is used to select the top k features based on a\n",
    "statistical test.\n",
    "\n",
    "# Define feature selection step (optional)\n",
    "feature_selector = SelectKBest(k='all')  # Choose 'k' based on your preference\n",
    "\n",
    "# Create the final preprocessing and feature selection pipeline\n",
    "final_preprocessor = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selector', feature_selector)  # Add or remove this step as needed\n",
    "])\n",
    "\n",
    "\n",
    "Step 5: Define the Model\n",
    "\n",
    "Define the machine learning model you want to use. In this example, we'll use a Random Forest Classifier.\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "\n",
    "Step 6: Create the Final Pipeline\n",
    "\n",
    "Combine the preprocessing and modeling steps into a final pipeline.\n",
    "\n",
    "Step 7: Fit and Evaluate\n",
    "\n",
    "Fit the final pipeline on the training data and evaluate it on the testing data.\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "accuracy = final_pipeline.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "accuracy = final_pipeline.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "This pipeline automates feature engineering and handles missing values. You can adjust the preprocessing steps, feature\n",
    "selection, and the model to best suit your project's needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f681fb-bc44-4bce-80bd-ba1c298c7e6e",
   "metadata": {},
   "source": [
    "## Q2. Bu#ld a p#pel#ne that #ncludes a random forest class#f#er and a log#st#c regress#on class#f#er, and then use a vot#ng class#f#er to comb#ne the#r pred#ct#ons. Tra#n the p#pel#ne on the #r#s dataset and evaluate #ts accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30934ebb-cbf5-4e12-ac2b-8972534b04a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Certainly! You can build a machine learning pipeline that automates feature engineering, handles missing values, and uses a \n",
    "Random Forest classifier for your project. Below are the code snippets and explanations for each step of the pipeline:\n",
    "\n",
    "Step 1: Automated Feature Selection\n",
    "\n",
    "    ~Automated feature selection helps identify important features. One common method is to use feature importance from an\n",
    "     ensemble tree-based model like Random Forest or XGBoost.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Feature selection using Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "# Select important features, e.g., top 10 features\n",
    "selected_features = X.columns[np.argsort(feature_importances)[::-1][:10]]\n",
    "\n",
    "\n",
    "Step 2: Numerical Pipeline\n",
    "\n",
    "In this step, you'll handle missing values and scale the numerical columns.\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a numerical pipeline\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with mean\n",
    "    ('scaler', StandardScaler())  # Scale the numerical features\n",
    "])\n",
    "\n",
    "# Fit and transform the selected numerical features\n",
    "X_train_num = numerical_pipeline.fit_transform(X_train[selected_features])\n",
    "X_test_num = numerical_pipeline.transform(X_test[selected_features])\n",
    "\n",
    "\n",
    "Step 3: Categorical Pipeline\n",
    "\n",
    "Here, you'll handle missing values and one-hot encode the categorical columns.\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create a categorical pipeline\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with most frequent\n",
    "    ('onehot', OneHotEncoder())  # One-hot encode the categorical features\n",
    "])\n",
    "\n",
    "# Fit and transform the remaining categorical features\n",
    "X_train_cat = categorical_pipeline.fit_transform(X_train.drop(columns=selected_features))\n",
    "X_test_cat = categorical_pipeline.transform(X_test.drop(columns=selected_features))\n",
    "\n",
    "\n",
    "Step 4: Combine Numerical and Categorical Pipelines\n",
    "\n",
    "Combine the transformed numerical and categorical features using ColumnTransformer.\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Combine numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, selected_features),\n",
    "        ('cat', categorical_pipeline, X.columns.difference(selected_features))\n",
    "    ])\n",
    "    \n",
    "# Fit and transform the preprocessor\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "Step 5: Random Forest Classifier\n",
    "\n",
    "Build the final model using a Random Forest Classifier.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model on the processed data\n",
    "clf.fit(X_train_processed, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = clf.score(X_test_processed, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "Interpretation and Possible Improvements:\n",
    "\n",
    "    ~The pipeline automates feature selection, handles missing values, and builds a Random Forest classifier for heart\n",
    "     disease prediction.\n",
    "    ~Feature selection is crucial for model performance, and we selected the top 10 important features based on Random\n",
    "     Forest feature importances.\n",
    "    ~Imputing missing values with the mean for numerical columns and the most frequent value for categorical columns is a \n",
    "     simple strategy. You may consider more advanced imputation techniques.\n",
    "    ~Scaling numerical features is essential for models like Random Forest.\n",
    "    ~One-hot encoding categorical features is a standard approach to handle them in machine learning models.\n",
    "    ~The Random Forest classifier is used as the final model.\n",
    "    \n",
    "Possible Improvements:\n",
    "\n",
    "1.Hyperparameter Tuning: Optimize the hyperparameters of the Random Forest classifier for better performance, potentially\n",
    "  using grid search or random search.\n",
    "\n",
    "2.Feature Engineering: Explore more advanced feature engineering techniques, such as creating interaction terms or\n",
    "  polynomial features.\n",
    "\n",
    "3.Model Interpretation: Utilize model interpretation techniques like SHAP values or partial dependence plots to gain \n",
    "  insights into how the model is making predictions.\n",
    "\n",
    "4.Handling Class Imbalance: If there's a class imbalance in your dataset, consider techniques like oversampling,\n",
    "  undersampling, or using class-weighted models.\n",
    "\n",
    "5.Ensembling: Experiment with ensemble methods like stacking to improve model performance further.\n",
    "\n",
    "6.Cross-Validation: Ensure that you use cross-validation during model evaluation to get a more robust estimate of model\n",
    "  performance.\n",
    "\n",
    "Remember that the choice of feature engineering and modeling techniques should be guided by domain knowledge and the \n",
    "specific characteristics of your dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
